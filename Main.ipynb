{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:27.941232Z",
     "start_time": "2021-01-06T13:12:10.999525Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:27.972872Z",
     "start_time": "2021-01-06T13:12:27.944227Z"
    }
   },
   "outputs": [],
   "source": [
    "# transforms of images\n",
    "image_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=300, scale=(0.8, 1.1)),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=300, scale=(0.8, 1.1)),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=300, scale=(0.8, 1.1)),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:27.987863Z",
     "start_time": "2021-01-06T13:12:27.976864Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 0.01\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:28.003819Z",
     "start_time": "2021-01-06T13:12:27.994842Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dir = \"./Train/\"\n",
    "val_dir = \"./Val/\"\n",
    "test_dir = \"./Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:30.400143Z",
     "start_time": "2021-01-06T13:12:29.944667Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"train\": torchvision.datasets.ImageFolder(train_dir, transform=image_transforms[\"train\"]),\n",
    "    \"val\": torchvision.datasets.ImageFolder(val_dir, transform=image_transforms[\"val\"]),\n",
    "    \"test\": torchvision.datasets.ImageFolder(test_dir, transform=image_transforms[\"test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:30.415883Z",
     "start_time": "2021-01-06T13:12:30.402506Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True),\n",
    "    \"val\": DataLoader(datasets[\"val\"], batch_size=BATCH_SIZE, shuffle=True),\n",
    "    \"test\": DataLoader(datasets[\"test\"], batch_size=BATCH_SIZE, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:30.679962Z",
     "start_time": "2021-01-06T13:12:30.660985Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloaders[\"train\"].dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:31.069244Z",
     "start_time": "2021-01-06T13:12:31.035787Z"
    }
   },
   "outputs": [],
   "source": [
    "# a self-defined pooling layer\n",
    "class AdaptiveConcataPool2d(nn.Module):\n",
    "    def __init__(self, size=None):\n",
    "        super(AdaptiveConcataPool2d, self).__init__()\n",
    "        size = size or (1, 1)\n",
    "        self.avgPooling = nn.AdaptiveAvgPool2d(size)\n",
    "        self.maxPooling = nn.AdaptiveMaxPool2d(size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.maxPooling(x), self.avgPooling(x)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:31.426613Z",
     "start_time": "2021-01-06T13:12:31.412649Z"
    }
   },
   "outputs": [],
   "source": [
    "# transfer learning: ResNet50\n",
    "def get_model():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.avgpool = AdaptiveConcataPool2d()\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 2),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:31.739094Z",
     "start_time": "2021-01-06T13:12:31.720151Z"
    }
   },
   "outputs": [],
   "source": [
    "# process of training and validating \n",
    "def train_val(model, device, train_loader, val_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_max_accuracy = 0.0\n",
    "    val_acc = 0.0\n",
    "    for batch_id, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*images.size(0)\n",
    "    train_loss = total_loss/len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            correct = pred.eq(labels.view_as(pred))\n",
    "            accuracy = torch.mean(correct.type(torch.FloatTensor))\n",
    "            val_acc += accuracy.item()*images.size(0)\n",
    "\n",
    "        val_loss = val_loss/len(val_loader.dataset)\n",
    "        val_acc = val_acc/len(val_loader.dataset)\n",
    "\n",
    "    return train_loss, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:31.971419Z",
     "start_time": "2021-01-06T13:12:31.962431Z"
    }
   },
   "outputs": [],
   "source": [
    "# process of testing\n",
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0.0\n",
    "    t = torch.tensor([]).to(device)\n",
    "    l = torch.tensor([]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            l = torch.cat((l, labels.view_as(predicted)), 0)\n",
    "            t = torch.cat((t, predicted), 0)    \n",
    "            \n",
    "        accuracy = metrics.accuracy_score(l.tolist(), t.tolist())\n",
    "        precision = np.mean(metrics.precision_score(l.tolist(), t.tolist(), average=None), axis=0)\n",
    "        recall = np.mean(metrics.recall_score(l.tolist(), t.tolist(), average=None), axis=0)\n",
    "        f1 = np.mean(metrics.f1_score(l.tolist(), t.tolist(), average=None), axis=0)\n",
    "    \n",
    "        avg_loss = total_loss/len(test_loader.dataset)\n",
    "        t = torch.tensor([]).to(device)\n",
    "        l = torch.tensor([]).to(device)\n",
    "        \n",
    "        return total_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:32.817659Z",
     "start_time": "2021-01-06T13:12:32.197117Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:48.166512Z",
     "start_time": "2021-01-06T13:12:32.820654Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:12:48.227829Z",
     "start_time": "2021-01-06T13:12:48.203683Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epochs(model, device, dataloaders, criterion, optimizer, epochs):\n",
    "    print(\"{0:>15}|{1:>15}|{2:>15}|{3:>15}|{4:>15}\".format(\n",
    "        \"Epoch\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"))\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, val_loss, val_acc = train_val(\n",
    "            model, device, dataloaders[\"train\"], dataloaders[\"val\"], optimizer, criterion, epoch)\n",
    "        test_loss, acc, pre, rec, fon = test(\n",
    "            model, device, dataloaders[\"test\"], criterion, epoch)\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "        print(\"{0:>15}|{1:>15}|{2:>15}|{3:>15}|{4:>15}\".format(epoch, acc, pre, rec, fon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T13:13:04.896870Z",
     "start_time": "2021-01-06T13:12:48.229823Z"
    }
   },
   "outputs": [],
   "source": [
    "train_epochs(model, device, dataloaders, criterion, optimizer, EPOCH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
